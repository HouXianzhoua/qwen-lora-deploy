from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

def main():
    model_path = "/root/qwen-lora-deploy/models/Qwen2-0.5B-Instruct"

    print(f"ğŸ” æ­£åœ¨ä» {model_path} åŠ è½½æ¨¡å‹ä¸åˆ†è¯å™¨...")
    
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True, torch_dtype=torch.float16)

    print("âœ… æ¨¡å‹ä¸åˆ†è¯å™¨åŠ è½½æˆåŠŸï¼")

    # ç®€å•æµ‹è¯•ç”Ÿæˆ
    prompt = "è¯·ç®€è¦ä»‹ç»ä¸€ä¸‹é‡å­åŠ›å­¦ã€‚"
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(inputs.input_ids.device)

    print("ğŸ§  æ¨¡å‹ç”Ÿæˆä¸­...")
    outputs = model.generate(**inputs, max_new_tokens=50)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    print("ğŸ“¢ æ¨¡å‹è¾“å‡ºï¼š")
    print(response)

if __name__ == "__main__":
    main()
